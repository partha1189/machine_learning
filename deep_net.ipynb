{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/partha1189/ml/blob/master/deep_net.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ot9ktG5wXrW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "4f3fe627-6180-4eef-a9dc-9d2388404d78"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"data\",one_hot = True)\n",
        "print(mnist)\n",
        "n_nodes_hl1 = 500\n",
        "n_nodes_hl2 = 500\n",
        "n_nodes_hl3 = 500\n",
        "\n",
        "n_classes = 10\n",
        "batch_size = 100\n",
        "\n",
        "#x = tf.placeholder('float',[None,784])\n",
        "#y = tf.placeholder('float')\n",
        "x = tf.placeholder(dtype=tf.float32,shape=[None,784], name='x')\n",
        "y = tf.placeholder(dtype=tf.float32,shape=[None,10], name= 'y')\n",
        "\n",
        "def neural_network_model(data):\n",
        "  hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784,n_nodes_hl1])),\n",
        "                   'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
        "  hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),\n",
        "                 'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
        "  hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),\n",
        "                 'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
        "  output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),\n",
        "                 'biases':tf.Variable(tf.random_normal([n_classes]))}\n",
        "  \n",
        "  l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']),hidden_1_layer['biases'])\n",
        "  l1 = tf.nn.relu(l1)\n",
        "  \n",
        "  l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']),hidden_2_layer['biases'])\n",
        "  l2 = tf.nn.relu(l2)\n",
        "  \n",
        "  l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']),hidden_3_layer['biases'])\n",
        "  l3 = tf.nn.relu(l3)\n",
        "  \n",
        "  output = tf.matmul(l3,output_layer['weights']) + output_layer['biases']\n",
        "  return output\n",
        "  \n",
        "def train_neural_network(x):\n",
        "  prediction = neural_network_model(x)\n",
        "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y,logits = prediction))\n",
        "  optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "  \n",
        "  hm_epochs = 10\n",
        "  \n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for epoch in range(hm_epochs):\n",
        "      epoch_loss = 0\n",
        "      \n",
        "      for _ in range(int(mnist.train.num_examples/batch_size)):\n",
        "        epoch_x,epoch_y = mnist.train.next_batch(batch_size)\n",
        "        _,c = sess.run([optimizer,cost],feed_dict = {x:epoch_x,y:epoch_y})\n",
        "        \n",
        "        epoch_loss += c\n",
        "      print('Epoch'+str(epoch) + 'completed out of '+str(hm_epochs)+' loss:'+str(epoch_loss))\n",
        "    correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
        "    \n",
        "    accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
        "    \n",
        "    print('Accuracy:',accuracy.eval({x:mnist.test.images,y:mnist.test.labels}))\n",
        "    \n",
        "train_neural_network(x)\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7feea25923c8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7feea2592278>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7feea2592390>)\n",
            "Epoch0completed out of 10 loss:1891101.6026916504\n",
            "Epoch1completed out of 10 loss:397784.7481533289\n",
            "Epoch2completed out of 10 loss:216361.67891085148\n",
            "Epoch3completed out of 10 loss:132888.00916973036\n",
            "Epoch4completed out of 10 loss:79391.60191959143\n",
            "Epoch5completed out of 10 loss:49380.829842482664\n",
            "Epoch6completed out of 10 loss:31327.69186537659\n",
            "Epoch7completed out of 10 loss:26144.093849650024\n",
            "Epoch8completed out of 10 loss:21897.724178152508\n",
            "Epoch9completed out of 10 loss:17794.302075074287\n",
            "Accuracy: 0.9503\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}